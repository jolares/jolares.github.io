"use strict";(self.webpackChunkai_blog=self.webpackChunkai_blog||[]).push([[9289],{876:(e,r,n)=>{n.d(r,{Zo:()=>s,kt:()=>y});var t=n(2784);function o(e,r,n){return r in e?Object.defineProperty(e,r,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[r]=n,e}function i(e,r){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);r&&(t=t.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),n.push.apply(n,t)}return n}function a(e){for(var r=1;r<arguments.length;r++){var n=null!=arguments[r]?arguments[r]:{};r%2?i(Object(n),!0).forEach((function(r){o(e,r,n[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(n,r))}))}return e}function c(e,r){if(null==e)return{};var n,t,o=function(e,r){if(null==e)return{};var n,t,o={},i=Object.keys(e);for(t=0;t<i.length;t++)n=i[t],r.indexOf(n)>=0||(o[n]=e[n]);return o}(e,r);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)n=i[t],r.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=t.createContext({}),p=function(e){var r=t.useContext(l),n=r;return e&&(n="function"==typeof e?e(r):a(a({},r),e)),n},s=function(e){var r=p(e.components);return t.createElement(l.Provider,{value:r},e.children)},f="mdxType",u={inlineCode:"code",wrapper:function(e){var r=e.children;return t.createElement(t.Fragment,{},r)}},m=t.forwardRef((function(e,r){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,s=c(e,["components","mdxType","originalType","parentName"]),f=p(n),m=o,y=f["".concat(l,".").concat(m)]||f[m]||u[m]||i;return n?t.createElement(y,a(a({ref:r},s),{},{components:n})):t.createElement(y,a({ref:r},s))}));function y(e,r){var n=arguments,o=r&&r.mdxType;if("string"==typeof e||o){var i=n.length,a=new Array(i);a[0]=m;var c={};for(var l in r)hasOwnProperty.call(r,l)&&(c[l]=r[l]);c.originalType=e,c[f]="string"==typeof e?e:o,a[1]=c;for(var p=2;p<i;p++)a[p]=n[p];return t.createElement.apply(null,a)}return t.createElement.apply(null,n)}m.displayName="MDXCreateElement"},6834:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>i,metadata:()=>c,toc:()=>p});var t=n(7896),o=(n(7294),n(876));const i={sidebar_position:4},a="Policy-Iteration",c={unversionedId:"reinforcement-learning/policy-iteration",id:"reinforcement-learning/policy-iteration",title:"Policy-Iteration",description:"",source:"@site/docs/reinforcement-learning/policy-iteration.md",sourceDirName:"reinforcement-learning",slug:"/reinforcement-learning/policy-iteration",permalink:"/docs/reinforcement-learning/policy-iteration",draft:!1,editUrl:"https://github.com/jolares/jolares.github.io/tree/master/apps/ai-blog/docs/docs/reinforcement-learning/policy-iteration.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"docsSidebar",previous:{title:"Inverse Reinforcement Learning (IRL)",permalink:"/docs/reinforcement-learning/inverse-reinforcement-learning"},next:{title:"Q-Learning",permalink:"/docs/reinforcement-learning/qlearning"}},l={},p=[],s={toc:p},f="wrapper";function u(e){let{components:r,...n}=e;return(0,o.kt)(f,(0,t.Z)({},s,n,{components:r,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"policy-iteration"},"Policy-Iteration"))}u.isMDXComponent=!0}}]);